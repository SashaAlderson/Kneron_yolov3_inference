{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone keras yolov3 repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1\n",
      "Cloning into 'keras_yolo3'...\n",
      "remote: Enumerating objects: 144, done.\u001b[K\n",
      "remote: Total 144 (delta 0), reused 0 (delta 0), pack-reused 144\u001b[K\n",
      "Receiving objects: 100% (144/144), 151.08 KiB | 836.00 KiB/s, done.\n",
      "Resolving deltas: 100% (65/65), done.\n"
     ]
    }
   ],
   "source": [
    "%cd /data1\n",
    "!git clone https://github.com/qqwweee/keras-yolo3.git keras_yolo3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and convert yolov3 model to keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1/keras_yolo3\n",
      "--2021-08-23 11:49:28--  https://pjreddie.com/media/files/yolov3.weights\n",
      "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
      "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 248007048 (237M) [application/octet-stream]\n",
      "Saving to: ‘yolov3.weights’\n",
      "\n",
      "yolov3.weights      100%[===================>] 236.52M  7.56MB/s    in 31s     \n",
      "\n",
      "2021-08-23 11:50:00 (7.62 MB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n",
      "\n",
      "/workspace/miniconda/lib/python3.7/site-packages/numpy/__init__.py:156: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n",
      "  from . import _distributor_init\n",
      "Using TensorFlow backend.\n",
      "Loading weights.\n",
      "Weights Header:  0 2 0 [32013312]\n",
      "Parsing Darknet config.\n",
      "Creating Keras model.\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Parsing section net_0\n",
      "Parsing section convolutional_0\n",
      "conv2d bn leaky (3, 3, 3, 32)\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2021-08-23 11:50:02.489433: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-08-23 11:50:02.493729: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz\n",
      "2021-08-23 11:50:02.494642: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563560b92230 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-08-23 11:50:02.494731: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "Parsing section convolutional_1\n",
      "conv2d bn leaky (3, 3, 32, 64)\n",
      "Parsing section convolutional_2\n",
      "conv2d bn leaky (1, 1, 64, 32)\n",
      "Parsing section convolutional_3\n",
      "conv2d bn leaky (3, 3, 32, 64)\n",
      "Parsing section shortcut_0\n",
      "Parsing section convolutional_4\n",
      "conv2d bn leaky (3, 3, 64, 128)\n",
      "Parsing section convolutional_5\n",
      "conv2d bn leaky (1, 1, 128, 64)\n",
      "Parsing section convolutional_6\n",
      "conv2d bn leaky (3, 3, 64, 128)\n",
      "Parsing section shortcut_1\n",
      "Parsing section convolutional_7\n",
      "conv2d bn leaky (1, 1, 128, 64)\n",
      "Parsing section convolutional_8\n",
      "conv2d bn leaky (3, 3, 64, 128)\n",
      "Parsing section shortcut_2\n",
      "Parsing section convolutional_9\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section convolutional_10\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_11\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_3\n",
      "Parsing section convolutional_12\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_13\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_4\n",
      "Parsing section convolutional_14\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_15\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_5\n",
      "Parsing section convolutional_16\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_17\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_6\n",
      "Parsing section convolutional_18\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_19\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_7\n",
      "Parsing section convolutional_20\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_21\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_8\n",
      "Parsing section convolutional_22\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_23\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_9\n",
      "Parsing section convolutional_24\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_25\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_10\n",
      "Parsing section convolutional_26\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section convolutional_27\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_28\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_11\n",
      "Parsing section convolutional_29\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_30\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_12\n",
      "Parsing section convolutional_31\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_32\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_13\n",
      "Parsing section convolutional_33\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_34\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_14\n",
      "Parsing section convolutional_35\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_36\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_15\n",
      "Parsing section convolutional_37\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_38\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_16\n",
      "Parsing section convolutional_39\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_40\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_17\n",
      "Parsing section convolutional_41\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_42\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_18\n",
      "Parsing section convolutional_43\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section convolutional_44\n",
      "conv2d bn leaky (1, 1, 1024, 512)\n",
      "Parsing section convolutional_45\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section shortcut_19\n",
      "Parsing section convolutional_46\n",
      "conv2d bn leaky (1, 1, 1024, 512)\n",
      "Parsing section convolutional_47\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section shortcut_20\n",
      "Parsing section convolutional_48\n",
      "conv2d bn leaky (1, 1, 1024, 512)\n",
      "Parsing section convolutional_49\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section shortcut_21\n",
      "Parsing section convolutional_50\n",
      "conv2d bn leaky (1, 1, 1024, 512)\n",
      "Parsing section convolutional_51\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section shortcut_22\n",
      "Parsing section convolutional_52\n",
      "conv2d bn leaky (1, 1, 1024, 512)\n",
      "Parsing section convolutional_53\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section convolutional_54\n",
      "conv2d bn leaky (1, 1, 1024, 512)\n",
      "Parsing section convolutional_55\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section convolutional_56\n",
      "conv2d bn leaky (1, 1, 1024, 512)\n",
      "Parsing section convolutional_57\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section convolutional_58\n",
      "conv2d    linear (1, 1, 1024, 255)\n",
      "Parsing section yolo_0\n",
      "Parsing section route_0\n",
      "Parsing section convolutional_59\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section upsample_0\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "Parsing section route_1\n",
      "Concatenating route layers: [<tf.Tensor 'up_sampling2d_1/ResizeNearestNeighbor:0' shape=(?, ?, ?, 256) dtype=float32>, <tf.Tensor 'add_19/add:0' shape=(?, ?, ?, 512) dtype=float32>]\n",
      "Parsing section convolutional_60\n",
      "conv2d bn leaky (1, 1, 768, 256)\n",
      "Parsing section convolutional_61\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section convolutional_62\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_63\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section convolutional_64\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_65\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section convolutional_66\n",
      "conv2d    linear (1, 1, 512, 255)\n",
      "Parsing section yolo_1\n",
      "Parsing section route_2\n",
      "Parsing section convolutional_67\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section upsample_1\n",
      "Parsing section route_3\n",
      "Concatenating route layers: [<tf.Tensor 'up_sampling2d_2/ResizeNearestNeighbor:0' shape=(?, ?, ?, 128) dtype=float32>, <tf.Tensor 'add_11/add:0' shape=(?, ?, ?, 256) dtype=float32>]\n",
      "Parsing section convolutional_68\n",
      "conv2d bn leaky (1, 1, 384, 128)\n",
      "Parsing section convolutional_69\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section convolutional_70\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_71\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section convolutional_72\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_73\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section convolutional_74\n",
      "conv2d    linear (1, 1, 256, 255)\n",
      "Parsing section yolo_2\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 3 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 3 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 3 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, None, None, 3 0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 6 18432       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 6 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 3 2048        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 3 128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 3 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 6 18432       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 6 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, None, 6 0           leaky_re_lu_2[0][0]              \n",
      "                                                                 leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, None, None, 6 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 1 73728       zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 1 512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 6 8192        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 6 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 1 73728       leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 1 512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, None, None, 1 0           leaky_re_lu_5[0][0]              \n",
      "                                                                 leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 6 8192        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 6 256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 1 73728       leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 1 512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, None, None, 1 0           add_2[0][0]                      \n",
      "                                                                 leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, None, None, 1 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 2 294912      zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 2 1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 1 512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 2 1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, None, None, 2 0           leaky_re_lu_10[0][0]             \n",
      "                                                                 leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 1 32768       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, 1 512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, None, 2 1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, None, None, 2 0           add_4[0][0]                      \n",
      "                                                                 leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 1 32768       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, None, 1 512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, None, 2 1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, None, None, 2 0           add_5[0][0]                      \n",
      "                                                                 leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 1 32768       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, None, 1 512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, None, 2 1024        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, None, None, 2 0           add_6[0][0]                      \n",
      "                                                                 leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 1 32768       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, None, 1 512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, None, 2 1024        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, None, None, 2 0           add_7[0][0]                      \n",
      "                                                                 leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, None, None, 1 32768       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, None, 1 512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, None, 2 1024        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, None, None, 2 0           add_8[0][0]                      \n",
      "                                                                 leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, None, None, 1 32768       add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, None, 1 512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, None, 2 1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, None, None, 2 0           add_9[0][0]                      \n",
      "                                                                 leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, None, None, 1 32768       add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, None, 1 512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, None, 2 1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, None, None, 2 0           add_10[0][0]                     \n",
      "                                                                 leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, None, None, 2 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, None, None, 5 1179648     zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, None, None, 5 2048        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, None, None, 2 1024        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, None, None, 5 2048        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, None, None, 5 0           leaky_re_lu_27[0][0]             \n",
      "                                                                 leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, None, None, 2 131072      add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, None, 2 1024        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, None, 5 2048        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, None, None, 5 0           add_12[0][0]                     \n",
      "                                                                 leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, None, None, 2 131072      add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, None, 2 1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, None, 5 2048        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, None, None, 5 0           add_13[0][0]                     \n",
      "                                                                 leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, None, None, 2 131072      add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, None, 2 1024        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, None, 5 2048        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, None, None, 5 0           add_14[0][0]                     \n",
      "                                                                 leaky_re_lu_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, None, None, 2 131072      add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, None, 2 1024        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, None, 5 2048        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, None, None, 5 0           add_15[0][0]                     \n",
      "                                                                 leaky_re_lu_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, None, None, 2 131072      add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, None, None, 2 1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, None, 5 2048        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, None, None, 5 0           add_16[0][0]                     \n",
      "                                                                 leaky_re_lu_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, None, None, 2 131072      add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, None, 2 1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, None, None, 5 2048        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, None, None, 5 0           add_17[0][0]                     \n",
      "                                                                 leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, None, None, 2 131072      add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, None, None, 2 1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, None, None, 5 2048        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, None, None, 5 0           add_18[0][0]                     \n",
      "                                                                 leaky_re_lu_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, None, None, 5 0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, None, None, 1 4718592     zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, None, None, 1 4096        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, None, None, 5 524288      leaky_re_lu_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, None, None, 5 2048        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, None, None, 1 4096        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, None, None, 1 0           leaky_re_lu_44[0][0]             \n",
      "                                                                 leaky_re_lu_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, None, None, 5 524288      add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, None, None, 5 2048        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, None, None, 1 4096        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, None, None, 1 0           add_20[0][0]                     \n",
      "                                                                 leaky_re_lu_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, None, None, 5 524288      add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, None, None, 5 2048        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, None, None, 1 4096        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, None, None, 1 0           add_21[0][0]                     \n",
      "                                                                 leaky_re_lu_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, None, None, 5 524288      add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, None, None, 5 2048        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, None, None, 1 4096        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, None, None, 1 0           add_22[0][0]                     \n",
      "                                                                 leaky_re_lu_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, None, None, 5 524288      add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, None, None, 5 2048        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_53[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_54 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_53[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_54 (BatchNo (None, None, None, 1 4096        conv2d_54[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_54 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_54[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_55 (Conv2D)              (None, None, None, 5 524288      leaky_re_lu_54[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_55 (BatchNo (None, None, None, 5 2048        conv2d_55[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_55 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_55[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_56 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_55[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_56 (BatchNo (None, None, None, 1 4096        conv2d_56[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_56 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_56[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_57 (Conv2D)              (None, None, None, 5 524288      leaky_re_lu_56[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_57 (BatchNo (None, None, None, 5 2048        conv2d_57[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_57 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_57[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_60 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_57[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_59 (BatchNo (None, None, None, 2 1024        conv2d_60[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_59 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_59[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "up_sampling2d_1 (UpSampling2D)  (None, None, None, 2 0           leaky_re_lu_59[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "concatenate_1 (Concatenate)     (None, None, None, 7 0           up_sampling2d_1[0][0]            \r\n",
      "                                                                 add_19[0][0]                     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_61 (Conv2D)              (None, None, None, 2 196608      concatenate_1[0][0]              \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_60 (BatchNo (None, None, None, 2 1024        conv2d_61[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_60 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_60[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_62 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_60[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_61 (BatchNo (None, None, None, 5 2048        conv2d_62[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_61 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_61[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_63 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_61[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_62 (BatchNo (None, None, None, 2 1024        conv2d_63[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_62 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_62[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_64 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_62[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_63 (BatchNo (None, None, None, 5 2048        conv2d_64[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_63 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_63[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_65 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_63[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_64 (BatchNo (None, None, None, 2 1024        conv2d_65[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_64 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_64[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_68 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_64[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_66 (BatchNo (None, None, None, 1 512         conv2d_68[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_66 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_66[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "up_sampling2d_2 (UpSampling2D)  (None, None, None, 1 0           leaky_re_lu_66[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "concatenate_2 (Concatenate)     (None, None, None, 3 0           up_sampling2d_2[0][0]            \r\n",
      "                                                                 add_11[0][0]                     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_69 (Conv2D)              (None, None, None, 1 49152       concatenate_2[0][0]              \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_67 (BatchNo (None, None, None, 1 512         conv2d_69[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_67 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_67[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_70 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_67[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_68 (BatchNo (None, None, None, 2 1024        conv2d_70[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_68 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_68[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_71 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_68[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_69 (BatchNo (None, None, None, 1 512         conv2d_71[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_69 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_69[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_72 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_69[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_70 (BatchNo (None, None, None, 2 1024        conv2d_72[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_70 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_70[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_73 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_70[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_71 (BatchNo (None, None, None, 1 512         conv2d_73[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_71 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_71[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_58 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_57[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_66 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_64[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_74 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_71[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_58 (BatchNo (None, None, None, 1 4096        conv2d_58[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_65 (BatchNo (None, None, None, 5 2048        conv2d_66[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "batch_normalization_72 (BatchNo (None, None, None, 2 1024        conv2d_74[0][0]                  \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_58 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_58[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_65 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_65[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "leaky_re_lu_72 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_72[0][0]     \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_59 (Conv2D)              (None, None, None, 2 261375      leaky_re_lu_58[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_67 (Conv2D)              (None, None, None, 2 130815      leaky_re_lu_65[0][0]             \r\n",
      "__________________________________________________________________________________________________\r\n",
      "conv2d_75 (Conv2D)              (None, None, None, 2 65535       leaky_re_lu_72[0][0]             \r\n",
      "==================================================================================================\r\n",
      "Total params: 62,001,757\r\n",
      "Trainable params: 61,949,149\r\n",
      "Non-trainable params: 52,608\r\n",
      "__________________________________________________________________________________________________\r\n",
      "None\r\n",
      "Saved Keras model to /data1/yolo.h5\r\n",
      "Read 62001757 of 62001757.0 from Darknet weights.\r\n"
     ]
    }
   ],
   "source": [
    "%cd keras_yolo3\n",
    "!wget https://pjreddie.com/media/files/yolov3.weights\n",
    "!python convert.py yolov3.cfg yolov3.weights /data1/yolo.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download kneron images for further quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1\n",
      "--2021-08-23 11:51:19--  http://doc.kneron.com/docs/toolchain/res/test_image10.zip\n",
      "Resolving doc.kneron.com (doc.kneron.com)... 75.26.5.105\n",
      "Connecting to doc.kneron.com (doc.kneron.com)|75.26.5.105|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2615052 (2.5M) [application/zip]\n",
      "Saving to: ‘test_image10.zip’\n",
      "\n",
      "test_image10.zip    100%[===================>]   2.49M  1.46MB/s    in 1.7s    \n",
      "\n",
      "2021-08-23 11:51:21 (1.46 MB/s) - ‘test_image10.zip’ saved [2615052/2615052]\n",
      "\n",
      "Archive:  test_image10.zip\n",
      "   creating: test_image10/\n",
      " extracting: test_image10/000000000139.jpg  \n",
      "  inflating: test_image10/000000000785.jpg  \n",
      "  inflating: test_image10/000000000872.jpg  \n",
      "  inflating: test_image10/000000000885.jpg  \n",
      " extracting: test_image10/000000001000.jpg  \n",
      "  inflating: test_image10/000000001268.jpg  \n",
      "  inflating: test_image10/000000001296.jpg  \n",
      "  inflating: test_image10/000000005001.jpg  \n",
      "  inflating: test_image10/000000005193.jpg  \n",
      "  inflating: test_image10/309_190.jpg  \n"
     ]
    }
   ],
   "source": [
    "%cd /data1\n",
    "!wget http://doc.kneron.com/docs/toolchain/res/test_image10.zip\n",
    "!unzip test_image10.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy image for model's testing in different convertation stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1\n"
     ]
    }
   ],
   "source": [
    "%cd /data1\n",
    "!cp /workspace/E2E_Simulator/app/test_image_folder/yolo/000000350003.jpg ./."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write and run example script test.py to convert yolov3 from keras to nef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "\n",
    "import ktc\n",
    "import os\n",
    "import onnx\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import sys\n",
    "sys.path.append(str(pathlib.Path(\"keras_yolo3\").resolve()))\n",
    "from yolo3.model import yolo_eval\n",
    "\n",
    "# Postprocess function of Yolov3\n",
    "def postprocess(inf_results, ori_image_shape):\n",
    "    tensor_data = [tf.convert_to_tensor(data, dtype=tf.float32) for data in inf_results]\n",
    "\n",
    "    # Get anchor info\n",
    "    anchors_path = \"/data1/keras_yolo3/model_data/yolo_anchors.txt\" \n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    anchors = np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "    # Postprocess\n",
    "    num_classes = 80\n",
    "    boxes, scores, classes = yolo_eval(tensor_data, anchors, num_classes, ori_image_shape)\n",
    "    with tf.Session() as sess:\n",
    "        boxes = boxes.eval()\n",
    "        scores = scores.eval()\n",
    "        classes = classes.eval()\n",
    "\n",
    "    return boxes, scores, classes\n",
    "\n",
    "from yolo3.utils import letterbox_image\n",
    "\n",
    "# Preprocess function of Yolov3 \n",
    "def preprocess(pil_img):\n",
    "    model_input_size = (416, 416)  # to match our model input size when converting\n",
    "    boxed_image = letterbox_image(pil_img, model_input_size)\n",
    "    np_data = np.array(boxed_image, dtype='float32')\n",
    "\n",
    "    np_data /= 255.\n",
    "    return np_data\n",
    "\n",
    "# Convert h5 model to onnx\n",
    "m = ktc.onnx_optimizer.keras2onnx_flow(\"/data1/yolo.h5\", input_shape = [1,416,416,3])\n",
    "m = ktc.onnx_optimizer.onnx2onnx_flow(m)\n",
    "onnx.save(m,'yolo.opt.onnx')\n",
    "\n",
    "\n",
    "# Npu(only) performance simulation\n",
    "km = ktc.ModelConfig(1001, \"0001\", \"520\", onnx_model=m)\n",
    "eval_result = km.evaluate()\n",
    "print(\"\\nNpu performance evaluation result:\\n\" + str(eval_result))\n",
    "\n",
    "\n",
    "\n",
    "# Onnx model check\n",
    "input_image = Image.open('/data1/000000350003.jpg')\n",
    "in_data = preprocess(input_image)\n",
    "out_data = ktc.kneron_inference([in_data], onnx_file=\"/data1/yolo.opt.onnx\", input_names=[\"input_1_o0\"])\n",
    "det_res = postprocess(out_data, [input_image.size[1], input_image.size[0]])\n",
    "print(det_res)\n",
    "\n",
    "# Load and normalize all image data from folder\n",
    "img_list = []\n",
    "for (dir_path, _, file_names) in os.walk(\"/data1/test_image10\"):\n",
    "    for f_n in file_names:\n",
    "        fullpath = os.path.join(dir_path, f_n)\n",
    "        print(\"processing image: \" + fullpath)\n",
    "\n",
    "        image = Image.open(fullpath)\n",
    "        img_data = preprocess(image)\n",
    "        img_list.append(img_data)\n",
    "\n",
    "\n",
    "# Fix point analysis\n",
    "bie_model_path = km.analysis({\"input_1_o0\": img_list})\n",
    "print(\"\\nFix point analysis done. Save bie model to '\" + str(bie_model_path) + \"'\")\n",
    "\n",
    "\n",
    "# Bie model check\n",
    "input_image = Image.open('/data1/000000350003.jpg')\n",
    "in_data = preprocess(input_image)\n",
    "out_data = ktc.kneron_inference([in_data], bie_file=bie_model_path, input_names=[\"input_1_o0\"])\n",
    "det_res = postprocess(out_data, [input_image.size[1], input_image.size[0]])\n",
    "print(det_res)\n",
    "\n",
    "\n",
    "# Compile\n",
    "nef_model_path = ktc.compile([km])\n",
    "print(\"\\nCompile done. Save Nef file to '\" + str(nef_model_path) + \"'\")\n",
    "\n",
    "# Nef model check\n",
    "input_image = Image.open('/data1/000000350003.jpg')\n",
    "in_data = preprocess(input_image)\n",
    "out_data = ktc.kneron_inference([in_data], nef_file=nef_model_path, radix=7)\n",
    "det_res = postprocess(out_data, [input_image.size[1], input_image.size[0]])\n",
    "print(det_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/miniconda/lib/python3.7/site-packages/numpy/__init__.py:156: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n",
      "  from . import _distributor_init\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "/workspace/miniconda/lib/python3.7/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "Currently, custom input size is only available for single input size. Mystery node may generate wrong size.\n",
      "running compiler and IP evaluator...\n",
      "Compiler config generated.\n",
      "Compilation and IP Evaluation finished.\n",
      "\n",
      "Npu performance evaluation result:\n",
      "***** Warning: this model has 1 CPU ops which may cause that the report's fps is different from the actual fps *****\n",
      "***** Warning: CPU ops types: KneronResize.\n",
      "\n",
      "[Evaluation Result]\n",
      "estimate FPS float = 1.39266\n",
      "total time = 718.052 ms\n",
      "total theoretical covolution time = 213.788 ms\n",
      "average DRAM bandwidth = 0.341753 GB/s\n",
      "MAC efficiency to total time = 29.7734 %\n",
      "MAC idle time = 153.815 ms\n",
      "MAC running time = 564.237 ms\n",
      "\n",
      "WARNING:tensorflow:From /workspace/miniconda/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "(array([[251.099  , 535.6055 , 298.40985, 551.92285],\n",
      "       [256.12146, 408.87692, 295.4361 , 424.1309 ],\n",
      "       [259.28464, 474.59253, 298.83353, 525.9219 ],\n",
      "       [238.99045, 233.12668, 309.90182, 364.7603 ]], dtype=float32), array([0.90185964, 0.8807101 , 0.9952549 , 0.87767255], dtype=float32), array([0, 0, 2, 7], dtype=int32))\n",
      "processing image: /data1/test_image10/000000000785.jpg\n",
      "processing image: /data1/test_image10/000000001268.jpg\n",
      "processing image: /data1/test_image10/000000000872.jpg\n",
      "processing image: /data1/test_image10/000000005001.jpg\n",
      "processing image: /data1/test_image10/309_190.jpg\n",
      "processing image: /data1/test_image10/000000000139.jpg\n",
      "processing image: /data1/test_image10/000000000885.jpg\n",
      "processing image: /data1/test_image10/000000005193.jpg\n",
      "processing image: /data1/test_image10/000000001296.jpg\n",
      "processing image: /data1/test_image10/000000001000.jpg\n",
      "[======================================================================]100% 0.038000s======================================>                         ]63% 0.017000s00s68% 0.021000s       ]73% 0.025000s ]61% 0.017000s========>                ]76% 0.027000s====================>                                            ]36% 0.007000s=====================================>             ]80% 0.030000s=========>     ]92% 0.036000s==========================>              ]79% 0.029000ss\n",
      "Fix point analysis done. Save bie model to '/data1/output.bie'\n",
      "(array([], shape=(0, 4), dtype=float32), array([], dtype=float32), array([], dtype=int32))\n",
      "\u001b[32m[tool][info][batch_compile.cc:543][BatchCompile] compiling output.bie\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:574][LayoutBins] Re-layout binaries\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:623][LayoutBins] output start: 0x601b8ef0, end: 0x601b8ef0\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:543][BatchCompile] compiling output.bie\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:733][CombineAllBin] Combine all bin files of all models into all_models.bin\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:809][WriteFwInfo] Generate firmware info to fw_info.txt & fw_info.bin\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:675][VerifyOutput] \n",
      "=> 1 models\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:683][VerifyOutput]      id: 1001\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:684][VerifyOutput]      version: 0x1\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:689][VerifyOutput]      addr: 0x60000000, size: 0xa9000\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:689][VerifyOutput]      addr: 0x600a9000, size: 0x10fef0\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:689][VerifyOutput]      addr: 0x601b8ef0, size: 0x7ec000\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:689][VerifyOutput]      addr: 0x609a4ef0, size: 0x1272c\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:689][VerifyOutput]      addr: 0x609b7620, size: 0x41d3ad0\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:689][VerifyOutput]      addr: 0x64b8b0f0, size: 0x2f8\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:692][VerifyOutput] \n",
      "\u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:696][VerifyOutput]   end addr 0x64b8b3e8, \u001b[m\n",
      "\u001b[32m[tool][info][batch_compile.cc:698][VerifyOutput] total bin size 0x41e64f8\u001b[m\n",
      "\n",
      "Compile done. Save Nef file to '/data1/batch_compile/models_520.nef'\n",
      "current node is an NPU INPUT NODE\n",
      "current node is a CPU NODE\n",
      "current node is an NPU INPUT NODE\n",
      "current node is a CPU NODE\n",
      "current node is an NPU INPUT NODE\n",
      "current node is a output NODE\n",
      "current node is a output NODE\n",
      "current node is a output NODE\n",
      "Info: set output buffer [600a9000, 600b5f30).\n",
      "Info: set output buffer [600b5f30, 600e9bf0).\n",
      "Info: set output buffer [600e9bf0, 601b8ef0).\n",
      "CSIM Version: 8ac3ec2\n",
      "---------- start npu ----------\n",
      "---------- start cpu ----------\n",
      "---- cpu node: cpu_op_type = Upsample\n",
      "---------- resume npu ----------\n",
      "---------- start cpu ----------\n",
      "---- cpu node: cpu_op_type = Upsample\n",
      "---------- resume npu ----------\n",
      "---------- dump output node ----------\n",
      "---------- dump output node ----------\n",
      "---------- dump output node ----------\n",
      "done\n",
      "(array([[252.54633, 408.06534, 298.7002 , 425.30664],\n",
      "       [251.45854, 535.3055 , 297.6124 , 552.54675],\n",
      "       [262.37845, 475.18   , 297.76306, 525.94916],\n",
      "       [240.0032 , 230.15079, 309.23395, 369.48395]], dtype=float32), array([0.8922923 , 0.85433   , 0.99431807, 0.8707535 ], dtype=float32), array([0, 0, 2, 7], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "!python test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make changes to E2E simulator in order to use it with multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /workspace/E2E_Simulator/python_flow/nef/nef.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /workspace/E2E_Simulator/python_flow/nef/nef.py\n",
    "\"\"\"\n",
    "Perform inference using an input NEF file.\n",
    "\"\"\"\n",
    "import ctypes\n",
    "import math\n",
    "import pathlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import python_flow.common.directory_manager as dm\n",
    "import python_flow.common.exceptions as exceptions\n",
    "import python_flow.utils.utils as utils\n",
    "\n",
    "FILE_DIR = pathlib.Path(__file__).resolve().parents[0]\n",
    "E2E_DIR = pathlib.Path(__file__).resolve().parents[2]\n",
    "NEF_LIB = ctypes.CDLL(FILE_DIR / \"libnef.so\")\n",
    "CSIM520 = str(E2E_DIR / \"python_flow/npu_sim\")\n",
    "CSIM720 = str(E2E_DIR / \"python_flow/npu_csim\")\n",
    "BASE_INI = str(E2E_DIR / \"python_flow/720.ini\")\n",
    "\n",
    "def advance(f, offset=0):\n",
    "    f.read(offset)\n",
    "    return int.from_bytes(f.read(4), byteorder='little')\n",
    "\n",
    "def parse_fw_info(path, key):\n",
    "    with open(path, 'rb') as f:\n",
    "        model_cnt = advance(f)\n",
    "\n",
    "        if \"model_cnt\" == key:\n",
    "            yield model_cnt\n",
    "            return\n",
    "\n",
    "        model_info_list = []\n",
    "        for _ in range(model_cnt):\n",
    "            model_info = {}\n",
    "            model_info['model_id'] = advance(f)\n",
    "            model_info['model_ver'] = hex(advance(f))\n",
    "\n",
    "            model_info['addr_in'] = hex(advance(f))\n",
    "            model_info['size_in'] = hex(advance(f))\n",
    "\n",
    "            model_info['addr_out'] = hex(advance(f))\n",
    "            model_info['size_out'] = hex(advance(f))\n",
    "\n",
    "            model_info['addr_wbuf'] = hex(advance(f))\n",
    "            model_info['size_wbuf'] = hex(advance(f))\n",
    "\n",
    "            model_info['addr_cmd'] = hex(advance(f))\n",
    "            model_info['size_cmd'] = hex(advance(f))\n",
    "\n",
    "            model_info['addr_wt'] = hex(advance(f))\n",
    "            model_info['size_wt'] = hex(advance(f))\n",
    "\n",
    "            model_info['addr_fw'] = hex(advance(f))\n",
    "            model_info['size_fw'] = hex(advance(f))\n",
    "\n",
    "            model_info_list.append(model_info)\n",
    "\n",
    "            if \"all\" != key:\n",
    "                yield model_info[key]\n",
    "\n",
    "        if \"all\" == key:\n",
    "            yield model_info_list\n",
    "            #print(model_info_list)\n",
    "\n",
    "def decompose_all_models(nef_file):\n",
    "    \"\"\"Separate the command/setup/weight binaries from the combined binary.\"\"\"\n",
    "    nef_folder = nef_file.parent\n",
    "    models = nef_folder / \"all_models.bin\"\n",
    "    fw_info = nef_folder / \"fw_info.bin\"\n",
    "    model_cnt = next(parse_fw_info(fw_info, \"model_cnt\"))\n",
    "    model_id_list = list(parse_fw_info(fw_info, \"model_id\"))\n",
    "    size_cmd_list = list(parse_fw_info(fw_info, \"size_cmd\"))\n",
    "    size_wt_list = list(parse_fw_info(fw_info, \"size_wt\"))\n",
    "    size_fw_list = list(parse_fw_info(fw_info, \"size_fw\"))\n",
    "\n",
    "    all_models = []\n",
    "    with open(models, \"r+b\") as af:\n",
    "        for idx in range(model_cnt):\n",
    "            cur_model = \"model_\" + str(model_id_list[idx])\n",
    "            all_models.append(cur_model)\n",
    "            with open(nef_folder / (cur_model + \"_command.bin\"), \"w+b\") as cbf:\n",
    "                size_cmd = int(size_cmd_list[idx], 16)\n",
    "                cbf.write(af.read(size_cmd))\n",
    "                cbf.close()\n",
    "                pad = math.ceil(size_cmd / 16.0) * 16 - size_cmd\n",
    "                af.read(pad) #pad to 16\n",
    "\n",
    "            with open(nef_folder / (cur_model + \"_weight.bin\"), \"w+b\") as wbf:\n",
    "                size_wt = int(size_wt_list[idx], 16)\n",
    "                assert 0 == (size_wt % 16), \"weight size should be aligned 16\"\n",
    "                wbf.write(af.read(size_wt))\n",
    "                wbf.close()\n",
    "\n",
    "            with open(nef_folder / (cur_model + \"_setup.bin\"), \"w+b\") as fbf:\n",
    "                size_fw = int(size_fw_list[idx], 16)\n",
    "                fbf.write(af.read(size_fw))\n",
    "                fbf.close()\n",
    "                pad = math.ceil(size_fw / 16.0) * 16 - size_fw\n",
    "                af.read(pad) #pad to 16\n",
    "\n",
    "    return all_models\n",
    "\n",
    "def parse_nef(nef_file):\n",
    "    \"\"\"Get the combined binary, firmware info, and platform from the NEF file in C.\"\"\"\n",
    "    c_function = NEF_LIB.parse_nef\n",
    "    c_function.argtypes = [ctypes.c_char_p]\n",
    "    c_function.restype = ctypes.c_int\n",
    "    platform = c_function(str(nef_file).encode())\n",
    "\n",
    "    if platform == -1:\n",
    "        raise exceptions.LibError(f\"Could not parse NEF file correctly: {nef_file}\")\n",
    "\n",
    "    all_models = decompose_all_models(nef_file)\n",
    "    return platform, all_models\n",
    "\n",
    "def setup_nef(nef, model_id, model = '0'):\n",
    "    \"\"\"Parses NEF model and does extra checks and setup.\"\"\"\n",
    "    if not nef.exists():\n",
    "        raise exceptions.InvalidInputError(f\"Input NEF file does not exist: {nef}\")\n",
    "\n",
    "    platform, all_models = parse_nef(nef)\n",
    "    platform = 520 if platform == 0 else 720\n",
    "\n",
    "    model_num = \"model_\" + str(model_id) # potential model to run\n",
    "    if len(all_models) == 1:\n",
    "        model_num = all_models[0]\n",
    "    elif model_num not in all_models:\n",
    "        raise exceptions.InvalidInputError(f\"Specified model ID not found in NEF. ID = {model_id}\")\n",
    "    input_folder = nef.parent / \"out/nef/inputs\" / model_num / model\n",
    "    #print(input_folder)\n",
    "    output_folder = nef.parent / \"out/nef/outputs\" / model_num / model\n",
    "    input_folder.mkdir(parents=True, exist_ok=True)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return platform, model_num, input_folder, output_folder\n",
    "\n",
    "def prep_csim_inputs(pre_results, input_folder, radix, platform):\n",
    "    \"\"\"Prepare the input files for NEF model.\"\"\"\n",
    "    input_files = []\n",
    "    for index, result in enumerate(pre_results):\n",
    "        file_name = \"_\".join([str(platform), \"csim_rgba_in\", str(index)]) + \".bin\"\n",
    "        input_file = str(input_folder / file_name)\n",
    "        input_files.append(input_file)\n",
    "        utils.convert_pre_numpy_to_rgba(result, input_file, radix, platform)\n",
    "\n",
    "    return input_files\n",
    "\n",
    "def update_ini(base_ini, new_ini, command, weight, setup, inputs):\n",
    "    \"\"\"Updates input INI file for the CSIM 720 model. Assume all files already exist.\"\"\"\n",
    "    file_inputs = \",\".join(inputs)\n",
    "    updated_lines = {\n",
    "        \"file_command\": \"\".join([\"file_command = \", command, \"\\n\"]),\n",
    "        \"file_weight\": \"\".join([\"file_weight = \", weight, \"\\n\"]),\n",
    "        \"file_setup\": \"\".join([\"file_setup = \", setup, \"\\n\"]),\n",
    "        \"file_input\": \"\".join([\"file_input = \", file_inputs, \"\\n\"])\n",
    "    }\n",
    "\n",
    "    new_file = []\n",
    "    with open(base_ini, \"r\") as in_file:\n",
    "        for line in in_file.readlines():\n",
    "            need_update = None\n",
    "            for key in updated_lines:\n",
    "                if line.startswith(key):\n",
    "                    need_update = key\n",
    "                    break\n",
    "\n",
    "            if need_update is not None:\n",
    "                new_file.append(updated_lines[need_update])\n",
    "                del updated_lines[need_update]\n",
    "            else:\n",
    "                new_file.append(line)\n",
    "\n",
    "    with open(new_ini, \"w\") as out_file:\n",
    "        out_file.write(\"\".join(new_file))\n",
    "\n",
    "def nef_inference(model_name, platform, data_type, nef_folder, input_files,\n",
    "                  input_folder, output_folder, reordering, ioinfo_file, dump, threads = 16):\n",
    "    \"\"\"Performs inference on the specified model parsed from the NEF file.\"\"\"\n",
    "    #print(model_name, platform, data_type,\"nef_folder: \", nef_folder,\"input_files: \", input_files,\n",
    "                  #\"input_folder: \", input_folder,\"output_folder\", output_folder,\"reordering: \", reordering,\"ioinfo_file: \", ioinfo_file, \"dump: \", dump)\n",
    "    command = str(nef_folder / \"_\".join([model_name, \"command.bin\"]))\n",
    "    setup = str(nef_folder / \"_\".join([model_name, \"setup.bin\"]))\n",
    "    weight = str(nef_folder / \"_\".join([model_name, \"weight.bin\"]))\n",
    "    with dm.DirectoryManager(output_folder):\n",
    "        try:\n",
    "            if platform == \"520\":\n",
    "                subprocess.run([CSIM520, \"-d\", dump, command, weight, *input_files, setup,\n",
    "                                \"--thread\", str(threads)], check=True)\n",
    "                output = utils.csim_520_to_np(\n",
    "                    str(output_folder), data_type, reordering, ioinfo_file, False)\n",
    "            else:\n",
    "                ini_file = input_folder / \"720.ini\"\n",
    "                update_ini(BASE_INI, ini_file, command, weight, setup, input_files)\n",
    "                subprocess.run([CSIM720, ini_file], check=True)\n",
    "                output = utils.csim_720_to_np(\n",
    "                    str(output_folder), data_type, reordering, ioinfo_file, False)\n",
    "        except subprocess.CalledProcessError as error:\n",
    "            raise exceptions.LibError(f\"Hardware CSIM {platform} failed:\\n{error}\")\n",
    "        except exceptions.ConfigError as error:\n",
    "            sys.exit(error)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /workspace/E2E_Simulator/python_flow/kneron_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /workspace/E2E_Simulator/python_flow/kneron_inference.py\n",
    "\"\"\"\n",
    "Generic inference function for ONNX, BIE, or NEF model.\n",
    "\"\"\"\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "ROOT_FOLDER = pathlib.Path(__file__).resolve().parent.parent\n",
    "sys.path.append(str(ROOT_FOLDER))\n",
    "\n",
    "import python_flow.common.exceptions as exceptions\n",
    "import python_flow.nef.nef as nef\n",
    "import python_flow.dynasty.dynasty as dynasty\n",
    "\n",
    "def kneron_inference(pre_results, nef_file=\"\", onnx_file=\"\", bie_file=\"\", model_id=None,\n",
    "                     input_names=[], radix=8, data_type=\"float\", reordering=[],\n",
    "                     ioinfo_file=\"\", dump=False, platform=520, model='0', threads = 16):\n",
    "    \"\"\"Performs inference on the input model given the specified parameters.\n",
    "\n",
    "    Input pre_results should be in format (1, h, w, c).\n",
    "\n",
    "    Arguments:\n",
    "        pre_results: List of NumPy arrays in channel last format from preprocessing\n",
    "        nef_file: Path to NEF model for inference\n",
    "        onnx_file: Path to ONNX model for inference, unused if nef_file is specified\n",
    "        bie_file: Path to BIE model for inference, unused if nef_file/onnx_file is specified\n",
    "        model_id: Integer of model to run inference, only necessary for NEF with multiple models\n",
    "        input_names: List of input node names of BIE/ONNX model, unused if nef_file is specified\n",
    "        radix: Integer radix to convert from float to fixed input\n",
    "        data_type: String format of the resulting output, \"fixed\" or \"float\"\n",
    "        dump: Boolean flag to dump intermediate nodes\n",
    "        reordering: List of node names/integers specifying the output order\n",
    "        ioinfo_file: String path to file mapping output node number to name, only used with NEF\n",
    "        platform: Integer indicating platform of Dynasty fixed model\n",
    "    \"\"\"\n",
    "    dump = 2 if dump else 0\n",
    "    if nef_file:\n",
    "        nef_path = pathlib.Path(nef_file).resolve()\n",
    "        #print(\"nef_path :\", nef_path)\n",
    "        platform, model_name, input_folder, output_folder = nef.setup_nef(nef_path, model_id, model)\n",
    "        #print(\"platform, model_name, input_folder, output_folder:\", platform, model_name, input_folder, output_folder)\n",
    "        input_files = nef.prep_csim_inputs(pre_results, input_folder, radix, str(platform))\n",
    "        #print(\"input_files: \", input_files)\n",
    "        output = nef.nef_inference(\n",
    "            model_name, str(platform), data_type, nef_path.parent, input_files, input_folder,\n",
    "            output_folder, reordering, ioinfo_file, str(dump), threads = 16)\n",
    "    elif onnx_file:\n",
    "        onnx = pathlib.Path(onnx_file).resolve()\n",
    "        input_folder = onnx.parent / \"out/onnx/inputs\" / onnx.name\n",
    "        output_folder = onnx.parent / \"out/onnx/outputs\" / onnx.name\n",
    "\n",
    "        input_files = dynasty.prep_dynasty(\n",
    "            pre_results, input_folder, output_folder, input_names, radix, platform, False)\n",
    "        output = dynasty.dynasty_inference(\n",
    "            onnx_file, \"Float\", str(platform), data_type, input_files, input_names,\n",
    "            str(output_folder), reordering, dump)\n",
    "    elif bie_file:\n",
    "        bie = pathlib.Path(bie_file).resolve()\n",
    "        input_folder = bie.parent / \"out/bie/inputs\" / bie.name\n",
    "        output_folder = bie.parent / \"out/bie/outputs\" / bie.name\n",
    "\n",
    "        input_files = dynasty.prep_dynasty(\n",
    "            pre_results, input_folder, output_folder, input_names, radix, platform, True)\n",
    "        output = dynasty.dynasty_inference(\n",
    "            bie_file, \"bie\", str(platform), data_type, input_files, input_names,\n",
    "            str(output_folder), reordering, dump)\n",
    "    else:\n",
    "        raise exceptions.RequiredConfigError(\"No input model selected for inference.\")\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /data1//keras_yolo3/yolo3/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /data1//keras_yolo3/yolo3/model.py\n",
    "\"\"\"YOLO_v3 Model Defined in Keras.\"\"\"\n",
    "\n",
    "from functools import wraps\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, Add, ZeroPadding2D, UpSampling2D, Concatenate, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/data1/keras_yolo3\")\n",
    "from yolo3.utils import compose\n",
    "\n",
    "\n",
    "@wraps(Conv2D)\n",
    "def DarknetConv2D(*args, **kwargs):\n",
    "    \"\"\"Wrapper to set Darknet parameters for Convolution2D.\"\"\"\n",
    "    darknet_conv_kwargs = {'kernel_regularizer': l2(5e-4)}\n",
    "    darknet_conv_kwargs['padding'] = 'valid' if kwargs.get('strides')==(2,2) else 'same'\n",
    "    darknet_conv_kwargs.update(kwargs)\n",
    "    return Conv2D(*args, **darknet_conv_kwargs)\n",
    "\n",
    "def DarknetConv2D_BN_Leaky(*args, **kwargs):\n",
    "    \"\"\"Darknet Convolution2D followed by BatchNormalization and LeakyReLU.\"\"\"\n",
    "    no_bias_kwargs = {'use_bias': False}\n",
    "    no_bias_kwargs.update(kwargs)\n",
    "    return compose(\n",
    "        DarknetConv2D(*args, **no_bias_kwargs),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1))\n",
    "\n",
    "def resblock_body(x, num_filters, num_blocks):\n",
    "    '''A series of resblocks starting with a downsampling Convolution2D'''\n",
    "    # Darknet uses left and top padding instead of 'same' mode\n",
    "    x = ZeroPadding2D(((1,0),(1,0)))(x)\n",
    "    x = DarknetConv2D_BN_Leaky(num_filters, (3,3), strides=(2,2))(x)\n",
    "    for i in range(num_blocks):\n",
    "        y = compose(\n",
    "                DarknetConv2D_BN_Leaky(num_filters//2, (1,1)),\n",
    "                DarknetConv2D_BN_Leaky(num_filters, (3,3)))(x)\n",
    "        x = Add()([x,y])\n",
    "    return x\n",
    "\n",
    "def darknet_body(x):\n",
    "    '''Darknent body having 52 Convolution2D layers'''\n",
    "    x = DarknetConv2D_BN_Leaky(32, (3,3))(x)\n",
    "    x = resblock_body(x, 64, 1)\n",
    "    x = resblock_body(x, 128, 2)\n",
    "    x = resblock_body(x, 256, 8)\n",
    "    x = resblock_body(x, 512, 8)\n",
    "    x = resblock_body(x, 1024, 4)\n",
    "    return x\n",
    "\n",
    "def make_last_layers(x, num_filters, out_filters):\n",
    "    '''6 Conv2D_BN_Leaky layers followed by a Conv2D_linear layer'''\n",
    "    x = compose(\n",
    "            DarknetConv2D_BN_Leaky(num_filters, (1,1)),\n",
    "            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\n",
    "            DarknetConv2D_BN_Leaky(num_filters, (1,1)),\n",
    "            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\n",
    "            DarknetConv2D_BN_Leaky(num_filters, (1,1)))(x)\n",
    "    y = compose(\n",
    "            DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),\n",
    "            DarknetConv2D(out_filters, (1,1)))(x)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def yolo_body(inputs, num_anchors, num_classes):\n",
    "    \"\"\"Create YOLO_V3 model CNN body in Keras.\"\"\"\n",
    "    darknet = Model(inputs, darknet_body(inputs))\n",
    "    x, y1 = make_last_layers(darknet.output, 512, num_anchors*(num_classes+5))\n",
    "\n",
    "    x = compose(\n",
    "            DarknetConv2D_BN_Leaky(256, (1,1)),\n",
    "            UpSampling2D(2))(x)\n",
    "    x = Concatenate()([x,darknet.layers[152].output])\n",
    "    x, y2 = make_last_layers(x, 256, num_anchors*(num_classes+5))\n",
    "\n",
    "    x = compose(\n",
    "            DarknetConv2D_BN_Leaky(128, (1,1)),\n",
    "            UpSampling2D(2))(x)\n",
    "    x = Concatenate()([x,darknet.layers[92].output])\n",
    "    x, y3 = make_last_layers(x, 128, num_anchors*(num_classes+5))\n",
    "\n",
    "    return Model(inputs, [y1,y2,y3])\n",
    "\n",
    "def tiny_yolo_body(inputs, num_anchors, num_classes):\n",
    "    '''Create Tiny YOLO_v3 model CNN body in keras.'''\n",
    "    x1 = compose(\n",
    "            DarknetConv2D_BN_Leaky(16, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(32, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(64, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(128, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(256, (3,3)))(inputs)\n",
    "    x2 = compose(\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(512, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(1,1), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(1024, (3,3)),\n",
    "            DarknetConv2D_BN_Leaky(256, (1,1)))(x1)\n",
    "    y1 = compose(\n",
    "            DarknetConv2D_BN_Leaky(512, (3,3)),\n",
    "            DarknetConv2D(num_anchors*(num_classes+5), (1,1)))(x2)\n",
    "\n",
    "    x2 = compose(\n",
    "            DarknetConv2D_BN_Leaky(128, (1,1)),\n",
    "            UpSampling2D(2))(x2)\n",
    "    y2 = compose(\n",
    "            Concatenate(),\n",
    "            DarknetConv2D_BN_Leaky(256, (3,3)),\n",
    "            DarknetConv2D(num_anchors*(num_classes+5), (1,1)))([x2,x1])\n",
    "\n",
    "    return Model(inputs, [y1,y2])\n",
    "\n",
    "\n",
    "def yolo_head(feats, anchors, num_classes, input_shape, calc_loss=False):\n",
    "    \"\"\"Convert final layer features to bounding box parameters.\"\"\"\n",
    "    num_anchors = len(anchors)\n",
    "    # Reshape to batch, height, width, num_anchors, box_params.\n",
    "    anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2])\n",
    "\n",
    "    grid_shape = K.shape(feats)[1:3] # height, width\n",
    "    grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]),\n",
    "        [1, grid_shape[1], 1, 1])\n",
    "    grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]),\n",
    "        [grid_shape[0], 1, 1, 1])\n",
    "    grid = K.concatenate([grid_x, grid_y])\n",
    "    grid = K.cast(grid, K.dtype(feats))\n",
    "\n",
    "    feats = K.reshape(\n",
    "        feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])\n",
    "\n",
    "    # Adjust preditions to each spatial grid point and anchor size.\n",
    "    box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[::-1], K.dtype(feats))\n",
    "    box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[::-1], K.dtype(feats))\n",
    "    box_confidence = K.sigmoid(feats[..., 4:5])\n",
    "    box_class_probs = K.sigmoid(feats[..., 5:])\n",
    "\n",
    "    if calc_loss == True:\n",
    "        return grid, feats, box_xy, box_wh\n",
    "    return box_xy, box_wh, box_confidence, box_class_probs\n",
    "\n",
    "\n",
    "def yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\n",
    "    '''Get corrected boxes'''\n",
    "    box_yx = box_xy[..., ::-1]\n",
    "    box_hw = box_wh[..., ::-1]\n",
    "    input_shape = K.cast(input_shape, K.dtype(box_yx))\n",
    "    image_shape = K.cast(image_shape, K.dtype(box_yx))\n",
    "    new_shape = K.round(image_shape * K.min(input_shape/image_shape))\n",
    "    offset = (input_shape-new_shape)/2./input_shape\n",
    "    scale = input_shape/new_shape\n",
    "    box_yx = (box_yx - offset) * scale\n",
    "    box_hw *= scale\n",
    "\n",
    "    box_mins = box_yx - (box_hw / 2.)\n",
    "    box_maxes = box_yx + (box_hw / 2.)\n",
    "    boxes =  K.concatenate([\n",
    "        box_mins[..., 0:1],  # y_min\n",
    "        box_mins[..., 1:2],  # x_min\n",
    "        box_maxes[..., 0:1],  # y_max\n",
    "        box_maxes[..., 1:2]  # x_max\n",
    "    ])\n",
    "\n",
    "    # Scale boxes back to original image shape.\n",
    "    boxes *= K.concatenate([image_shape, image_shape])\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def yolo_boxes_and_scores(feats, anchors, num_classes, input_shape, image_shape):\n",
    "    '''Process Conv layer output'''\n",
    "    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats,\n",
    "        anchors, num_classes, input_shape)\n",
    "    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n",
    "    boxes = K.reshape(boxes, [-1, 4])\n",
    "    box_scores = box_confidence * box_class_probs\n",
    "    box_scores = K.reshape(box_scores, [-1, num_classes])\n",
    "    return boxes, box_scores\n",
    "\n",
    "\n",
    "def yolo_eval(yolo_outputs,\n",
    "              anchors,\n",
    "              num_classes,\n",
    "              image_shape,\n",
    "              max_boxes=20,\n",
    "              score_threshold=.6,\n",
    "              iou_threshold=.5):\n",
    "    \"\"\"Evaluate YOLO model on given input and return filtered boxes.\"\"\"\n",
    "    num_layers = len(yolo_outputs)\n",
    "    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] # default setting\n",
    "    input_shape = K.shape(yolo_outputs[0])[1:3] * 32\n",
    "    boxes = []\n",
    "    box_scores = []\n",
    "    for l in range(num_layers):\n",
    "        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l],\n",
    "            anchors[anchor_mask[l]], num_classes, input_shape, image_shape)\n",
    "        boxes.append(_boxes)\n",
    "        box_scores.append(_box_scores)\n",
    "    boxes = K.concatenate(boxes, axis=0)\n",
    "    box_scores = K.concatenate(box_scores, axis=0)\n",
    "\n",
    "    mask = box_scores >= score_threshold\n",
    "    max_boxes_tensor = K.constant(max_boxes, dtype='int32')\n",
    "    boxes_ = []\n",
    "    scores_ = []\n",
    "    classes_ = []\n",
    "    for c in range(num_classes):\n",
    "        # TODO: use keras backend instead of tf.\n",
    "        class_boxes = tf.boolean_mask(boxes, mask[:, c])\n",
    "        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])\n",
    "        nms_index = tf.image.non_max_suppression(\n",
    "            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)\n",
    "        class_boxes = K.gather(class_boxes, nms_index)\n",
    "        class_box_scores = K.gather(class_box_scores, nms_index)\n",
    "        classes = K.ones_like(class_box_scores, 'int32') * c\n",
    "        boxes_.append(class_boxes)\n",
    "        scores_.append(class_box_scores)\n",
    "        classes_.append(classes)\n",
    "    boxes_ = K.concatenate(boxes_, axis=0)\n",
    "    scores_ = K.concatenate(scores_, axis=0)\n",
    "    classes_ = K.concatenate(classes_, axis=0)\n",
    "\n",
    "    return boxes_, scores_, classes_\n",
    "\n",
    "\n",
    "def preprocess_true_boxes(true_boxes, input_shape, anchors, num_classes):\n",
    "    '''Preprocess true boxes to training input format\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    true_boxes: array, shape=(m, T, 5)\n",
    "        Absolute x_min, y_min, x_max, y_max, class_id relative to input_shape.\n",
    "    input_shape: array-like, hw, multiples of 32\n",
    "    anchors: array, shape=(N, 2), wh\n",
    "    num_classes: integer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_true: list of array, shape like yolo_outputs, xywh are reletive value\n",
    "\n",
    "    '''\n",
    "    assert (true_boxes[..., 4]<num_classes).all(), 'class id must be less than num_classes'\n",
    "    num_layers = len(anchors)//3 # default setting\n",
    "    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]]\n",
    "\n",
    "    true_boxes = np.array(true_boxes, dtype='float32')\n",
    "    input_shape = np.array(input_shape, dtype='int32')\n",
    "    boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2\n",
    "    boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2]\n",
    "    true_boxes[..., 0:2] = boxes_xy/input_shape[::-1]\n",
    "    true_boxes[..., 2:4] = boxes_wh/input_shape[::-1]\n",
    "\n",
    "    m = true_boxes.shape[0]\n",
    "    grid_shapes = [input_shape//{0:32, 1:16, 2:8}[l] for l in range(num_layers)]\n",
    "    y_true = [np.zeros((m,grid_shapes[l][0],grid_shapes[l][1],len(anchor_mask[l]),5+num_classes),\n",
    "        dtype='float32') for l in range(num_layers)]\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    anchors = np.expand_dims(anchors, 0)\n",
    "    anchor_maxes = anchors / 2.\n",
    "    anchor_mins = -anchor_maxes\n",
    "    valid_mask = boxes_wh[..., 0]>0\n",
    "\n",
    "    for b in range(m):\n",
    "        # Discard zero rows.\n",
    "        wh = boxes_wh[b, valid_mask[b]]\n",
    "        if len(wh)==0: continue\n",
    "        # Expand dim to apply broadcasting.\n",
    "        wh = np.expand_dims(wh, -2)\n",
    "        box_maxes = wh / 2.\n",
    "        box_mins = -box_maxes\n",
    "\n",
    "        intersect_mins = np.maximum(box_mins, anchor_mins)\n",
    "        intersect_maxes = np.minimum(box_maxes, anchor_maxes)\n",
    "        intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "        box_area = wh[..., 0] * wh[..., 1]\n",
    "        anchor_area = anchors[..., 0] * anchors[..., 1]\n",
    "        iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
    "\n",
    "        # Find best anchor for each true box\n",
    "        best_anchor = np.argmax(iou, axis=-1)\n",
    "\n",
    "        for t, n in enumerate(best_anchor):\n",
    "            for l in range(num_layers):\n",
    "                if n in anchor_mask[l]:\n",
    "                    i = np.floor(true_boxes[b,t,0]*grid_shapes[l][1]).astype('int32')\n",
    "                    j = np.floor(true_boxes[b,t,1]*grid_shapes[l][0]).astype('int32')\n",
    "                    k = anchor_mask[l].index(n)\n",
    "                    c = true_boxes[b,t, 4].astype('int32')\n",
    "                    y_true[l][b, j, i, k, 0:4] = true_boxes[b,t, 0:4]\n",
    "                    y_true[l][b, j, i, k, 4] = 1\n",
    "                    y_true[l][b, j, i, k, 5+c] = 1\n",
    "\n",
    "    return y_true\n",
    "\n",
    "\n",
    "def box_iou(b1, b2):\n",
    "    '''Return iou tensor\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    b1: tensor, shape=(i1,...,iN, 4), xywh\n",
    "    b2: tensor, shape=(j, 4), xywh\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    iou: tensor, shape=(i1,...,iN, j)\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    b1 = K.expand_dims(b1, -2)\n",
    "    b1_xy = b1[..., :2]\n",
    "    b1_wh = b1[..., 2:4]\n",
    "    b1_wh_half = b1_wh/2.\n",
    "    b1_mins = b1_xy - b1_wh_half\n",
    "    b1_maxes = b1_xy + b1_wh_half\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    b2 = K.expand_dims(b2, 0)\n",
    "    b2_xy = b2[..., :2]\n",
    "    b2_wh = b2[..., 2:4]\n",
    "    b2_wh_half = b2_wh/2.\n",
    "    b2_mins = b2_xy - b2_wh_half\n",
    "    b2_maxes = b2_xy + b2_wh_half\n",
    "\n",
    "    intersect_mins = K.maximum(b1_mins, b2_mins)\n",
    "    intersect_maxes = K.minimum(b1_maxes, b2_maxes)\n",
    "    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    b1_area = b1_wh[..., 0] * b1_wh[..., 1]\n",
    "    b2_area = b2_wh[..., 0] * b2_wh[..., 1]\n",
    "    iou = intersect_area / (b1_area + b2_area - intersect_area)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def yolo_loss(args, anchors, num_classes, ignore_thresh=.5, print_loss=False):\n",
    "    '''Return yolo_loss tensor\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yolo_outputs: list of tensor, the output of yolo_body or tiny_yolo_body\n",
    "    y_true: list of array, the output of preprocess_true_boxes\n",
    "    anchors: array, shape=(N, 2), wh\n",
    "    num_classes: integer\n",
    "    ignore_thresh: float, the iou threshold whether to ignore object confidence loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss: tensor, shape=(1,)\n",
    "\n",
    "    '''\n",
    "    num_layers = len(anchors)//3 # default setting\n",
    "    yolo_outputs = args[:num_layers]\n",
    "    y_true = args[num_layers:]\n",
    "    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]]\n",
    "    input_shape = K.cast(K.shape(yolo_outputs[0])[1:3] * 32, K.dtype(y_true[0]))\n",
    "    grid_shapes = [K.cast(K.shape(yolo_outputs[l])[1:3], K.dtype(y_true[0])) for l in range(num_layers)]\n",
    "    loss = 0\n",
    "    m = K.shape(yolo_outputs[0])[0] # batch size, tensor\n",
    "    mf = K.cast(m, K.dtype(yolo_outputs[0]))\n",
    "\n",
    "    for l in range(num_layers):\n",
    "        object_mask = y_true[l][..., 4:5]\n",
    "        true_class_probs = y_true[l][..., 5:]\n",
    "\n",
    "        grid, raw_pred, pred_xy, pred_wh = yolo_head(yolo_outputs[l],\n",
    "             anchors[anchor_mask[l]], num_classes, input_shape, calc_loss=True)\n",
    "        pred_box = K.concatenate([pred_xy, pred_wh])\n",
    "\n",
    "        # Darknet raw box to calculate loss.\n",
    "        raw_true_xy = y_true[l][..., :2]*grid_shapes[l][::-1] - grid\n",
    "        raw_true_wh = K.log(y_true[l][..., 2:4] / anchors[anchor_mask[l]] * input_shape[::-1])\n",
    "        raw_true_wh = K.switch(object_mask, raw_true_wh, K.zeros_like(raw_true_wh)) # avoid log(0)=-inf\n",
    "        box_loss_scale = 2 - y_true[l][...,2:3]*y_true[l][...,3:4]\n",
    "\n",
    "        # Find ignore mask, iterate over each of batch.\n",
    "        ignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True)\n",
    "        object_mask_bool = K.cast(object_mask, 'bool')\n",
    "        def loop_body(b, ignore_mask):\n",
    "            true_box = tf.boolean_mask(y_true[l][b,...,0:4], object_mask_bool[b,...,0])\n",
    "            iou = box_iou(pred_box[b], true_box)\n",
    "            best_iou = K.max(iou, axis=-1)\n",
    "            ignore_mask = ignore_mask.write(b, K.cast(best_iou<ignore_thresh, K.dtype(true_box)))\n",
    "            return b+1, ignore_mask\n",
    "        _, ignore_mask = K.control_flow_ops.while_loop(lambda b,*args: b<m, loop_body, [0, ignore_mask])\n",
    "        ignore_mask = ignore_mask.stack()\n",
    "        ignore_mask = K.expand_dims(ignore_mask, -1)\n",
    "\n",
    "        # K.binary_crossentropy is helpful to avoid exp overflow.\n",
    "        xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[...,0:2], from_logits=True)\n",
    "        wh_loss = object_mask * box_loss_scale * 0.5 * K.square(raw_true_wh-raw_pred[...,2:4])\n",
    "        confidence_loss = object_mask * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True)+ \\\n",
    "            (1-object_mask) * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True) * ignore_mask\n",
    "        class_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[...,5:], from_logits=True)\n",
    "\n",
    "        xy_loss = K.sum(xy_loss) / mf\n",
    "        wh_loss = K.sum(wh_loss) / mf\n",
    "        confidence_loss = K.sum(confidence_loss) / mf\n",
    "        class_loss = K.sum(class_loss) / mf\n",
    "        loss += xy_loss + wh_loss + confidence_loss + class_loss\n",
    "        if print_loss:\n",
    "            loss = tf.Print(loss, [loss, xy_loss, wh_loss, confidence_loss, class_loss, K.sum(ignore_mask)], message='loss: ')\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /data1/parallel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /data1/parallel.py\n",
    "\n",
    "import argparse\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from time import time\n",
    "from time import sleep\n",
    "\n",
    "from keras_yolo3.yolo3.model import yolo_eval\n",
    "from keras_yolo3.yolo3.utils import letterbox_image\n",
    "\n",
    "import ktc\n",
    "import tensorflow as tf\n",
    "import onnx\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def postprocess(inf_results, ori_image_shape, conf_t  , iou_t ):\n",
    "    tensor_data = [tf.convert_to_tensor(data, dtype=tf.float32) for data in inf_results]\n",
    "\n",
    "    # Get anchor info\n",
    "    anchors_path = \"/data1/keras_yolo3/model_data/yolo_anchors.txt\"\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    anchors = np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "    # Postprocess of Yolov3\n",
    "    num_classes = 80\n",
    "    boxes, scores, classes = yolo_eval(tensor_data, anchors, num_classes, ori_image_shape, score_threshold=conf_t, iou_threshold=iou_t)\n",
    "    with tf.Session() as sess:\n",
    "        boxes = boxes.eval()\n",
    "        scores = scores.eval()\n",
    "        classes = classes.eval()\n",
    "\n",
    "    return boxes, scores, classes\n",
    "\n",
    "# Preprocess of Yolov3\n",
    "def preprocess(pil_img, img_size):    \n",
    "    model_input_size = (img_size, img_size)  # to match our model input size when converting\n",
    "    boxed_image = letterbox_image(pil_img, model_input_size)\n",
    "    np_data = np.array(boxed_image, dtype='float32')\n",
    "\n",
    "    np_data /= 255.\n",
    "    return np_data\n",
    "\n",
    "\n",
    "def setup_parser(test_args):\n",
    "    \"\"\"Setup the command line parser.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Runs an inference on multiple images.\",\n",
    "                                     formatter_class=argparse.RawTextHelpFormatter)\n",
    "    parser.add_argument(\"--demo\",action=\"store_true\", help=\"run demo on your image\")\n",
    "    parser.add_argument(\"--image\", help=\"path to your image for demo\", default=\"/data1/000000350003.jpg\", type=str)\n",
    "    parser.add_argument(\"--path\", help=\"directory of your images\", default=\"/workspace/COCO/valid2017\", type=str)\n",
    "    parser.add_argument(\"--nef\", help=\"path to your nef model\", default=\"/data1/batch_compile/models_520.nef\", type=str)    \n",
    "    parser.add_argument(\"--step\", help=\"number of images for one model in every step\", default=20, type=int)\n",
    "    parser.add_argument(\"--init\", help=\"initialization time between models\", default=10, type=int)\n",
    "    parser.add_argument(\"--model\", help=\"model's number\", default=0, type=int)\n",
    "    parser.add_argument(\"--threads\", help=\"choose number of workers for inference(only for 520 model)\", default=16, type=int)\n",
    "    parser.add_argument(\"--img-size\", help=\"image size\", default=416, type=int)\n",
    "    parser.add_argument(\"--conf-t\", help=\"confidence threshold\", default=0.6, type=float) \n",
    "    parser.add_argument(\"--iou-t\", help=\"iou threshold for NMS\", default=0.5, type=float)\n",
    "    #TODO add anchors\n",
    "    return parser.parse_args(test_args)\n",
    "        \n",
    "\n",
    "def main(test_args = None):\n",
    "    args = setup_parser(test_args)\n",
    "    sleep(int(args.model)*args.init)\n",
    "    step = args.step\n",
    "    \n",
    "    # Run demo on one image\n",
    "    if args.demo:\n",
    "        input_image = Image.open(args.image)\n",
    "        print(\"Image is in: \", args.image)\n",
    "        image = preprocess(input_image, args.img_size)\n",
    "        \n",
    "        tic = time()\n",
    "        out_data = ktc.kneron_inference([image], nef_file=args.nef, radix=7 , model=str(args.model), threads=args.threads)\n",
    "        print(f'inference time {(time()-tic):.2f}')\n",
    "\n",
    "        # Postprocess\n",
    "        tic = time()\n",
    "        det_res = postprocess(out_data, [input_image.size[1], input_image.size[0]], args.conf_t  , args.iou_t)\n",
    "        print(f'postprocess time {(time()-tic):.2f}')\n",
    "        print(det_res)\n",
    "        \n",
    "    # Make prediction on multiple images with multiple models    \n",
    "    else:\n",
    "        images_folder = args.path + \"/images\"\n",
    "        images = [f for f in listdir(images_folder) if isfile(join(images_folder, f))]\n",
    "        predictions_folder = args.path + \"/\" + \"predictions\"\n",
    "        images_folder = args.path + \"/\" + \"images\"\n",
    "        in_data = []\n",
    "        \n",
    "        # Check intersection between predictions and images      \n",
    "        predictions_name =[f[:12] for f in listdir(predictions_folder) if isfile(join(predictions_folder, f))]\n",
    "        images_name =[f[:12] for f in listdir(images_folder) if isfile(join(images_folder, f))]\n",
    "        predicted = list(set(predictions_name) & set(images_name))\n",
    "        not_predicted = images_name\n",
    "\n",
    "        # Remove already made predictions\n",
    "        for element in predicted:                \n",
    "            not_predicted.remove(element)\n",
    "\n",
    "        # Create list with images without predictions    \n",
    "        not_predicted_images = []\n",
    "        for elem in not_predicted:\n",
    "            not_predicted_images.append(elem + \".jpg\")\n",
    "            \n",
    "        image_shape = [] # make list to fill with image sizes\n",
    "        \n",
    "        # Preprocess *step* images \n",
    "        for i in range(step):\n",
    "            try:\n",
    "                input_image = Image.open(args.path + \"/images/\" + not_predicted_images[i+args.model*step])\n",
    "                pre = preprocess(input_image, args.img_size)\n",
    "                image_shape.append(input_image.size)\n",
    "                in_data.append(pre)\n",
    "            except:\n",
    "                print(\"No more images for model \", args.model)\n",
    "                break               \n",
    "\n",
    "        images = not_predicted_images\n",
    "                   \n",
    "        print(\"-\"*10)\n",
    "        print(\"Preprocessing is done!\")\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        count = 0 # to assign image to prediction\n",
    "        \n",
    "        # Make prediction \n",
    "        for image in in_data:\n",
    "            # Inference\n",
    "            tic = time()\n",
    "            out_data = ktc.kneron_inference([image], nef_file=args.nef, radix=7 , model=str(args.model), threads=args.threads)\n",
    "            print(f'inference time {(time()-tic):.2f}')\n",
    "            \n",
    "            # Postprocess\n",
    "            tic = time()\n",
    "            det_res = postprocess(out_data, [image_shape[count][1], image_shape[count][0]], args.conf_t, args.iou_t)\n",
    "            print(f'postprocess time {(time()-tic):.2f}') \n",
    "            \n",
    "            # Write predictions in YOLO style\n",
    "            predictions = [] # create tuple to fill with predictions\n",
    "            for pred in range(len(det_res[2])):\n",
    "                # Convert y1x1y2x2 to XcYcwh with relative values\n",
    "                prediction = (str(det_res[2][pred]) + \" \" + str(det_res[1][pred]) + \" \" \n",
    "                    + str((det_res[0][pred][1] + det_res[0][pred][3])/2/input_image.size[0])+ \" \" \n",
    "                    + str((det_res[0][pred][0] + det_res[0][pred][2])/2/input_image.size[1])+ \" \" \n",
    "                    + str((det_res[0][pred][3] - det_res[0][pred][1])/input_image.size[0])+ \" \" \n",
    "                    + str((det_res[0][pred][2] - det_res[0][pred][0])/input_image.size[1]))\n",
    "\n",
    "                predictions.append(prediction)\n",
    "                \n",
    "            predictions = '\\n'.join(predictions)\n",
    "\n",
    "            # Save predictions to directory args.path/predictions/*.txt\n",
    "            with open(predictions_folder + \"/\" + images[count+args.model*step][:12] + \".txt\", 'w') as f:\n",
    "                f.write(str(predictions))\n",
    "\n",
    "            count += 1\n",
    "          \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/COCO\n"
     ]
    }
   ],
   "source": [
    "!mkdir /workspace/COCO \n",
    "%cd /workspace/COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-23 12:01:35--  http://images.cocodataset.org/zips/val2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.135.17\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.135.17|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 815585330 (778M) [application/zip]\n",
      "Saving to: ‘val2017.zip’\n",
      "\n",
      "val2017.zip         100%[===================>] 777.80M  6.45MB/s    in 2m 4s   \n",
      "\n",
      "2021-08-23 12:03:40 (6.27 MB/s) - ‘val2017.zip’ saved [815585330/815585330]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -c http://images.cocodataset.org/zips/val2017.zip\n",
    "!unzip -q val2017.zip\n",
    "!rm val2017.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv val2017 images \n",
    "!mkdir val2017\n",
    "!mv images val2017/images\n",
    "!mkdir /workspace/COCO/val2017/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolov3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/miniconda/lib/python3.7/site-packages/numpy/__init__.py:156: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n",
      "  from . import _distributor_init\n",
      "Using TensorFlow backend.\n",
      "usage: parallel.py [-h] [--demo] [--image IMAGE] [--path PATH] [--nef NEF]\n",
      "                   [--step STEP] [--init INIT] [--model MODEL]\n",
      "                   [--threads THREADS] [--img-size IMG_SIZE] [--conf-t CONF_T]\n",
      "                   [--iou-t IOU_T]\n",
      "\n",
      "Runs an inference on multiple images.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help           show this help message and exit\n",
      "  --demo               run demo on your image\n",
      "  --image IMAGE        path to your image for demo\n",
      "  --path PATH          directory of your images\n",
      "  --nef NEF            path to your nef model\n",
      "  --step STEP          number of images for one model in every step\n",
      "  --init INIT          initialization time between models\n",
      "  --model MODEL        model's number\n",
      "  --threads THREADS    choose number of workers for inference(only for 520 model)\n",
      "  --img-size IMG_SIZE  image size\n",
      "  --conf-t CONF_T      confidence threshold\n",
      "  --iou-t IOU_T        iou threshold for NMS\n"
     ]
    }
   ],
   "source": [
    "!python /data1/parallel.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script will run untill no more predictions to be done.\n",
    "We recommend to increase number of models, if you're using it to simulate Kneron 720, \n",
    "since this simulator uses only one thread.\n",
    "\"\"\"\n",
    "\n",
    "import multiprocessing\n",
    "import os\n",
    "from time import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "tic = time() \n",
    "step = 5 # images per iteration of script\n",
    "models = 5 # number of models running in parallel\n",
    "init = 3 # time between model's initialization\n",
    "threads = 16 # number of workers for each model, works only for 520 model\n",
    "path = '/workspace/COCO/val2017' # path to validation directory\n",
    "predictions = path + '/' + 'predictions'\n",
    "labels= path + '/' +  'images'\n",
    "\n",
    "\n",
    "def missed(predictions, labels):    \n",
    "    predictions =[f[:12] for f in listdir(predictions) if isfile(join(predictions, f))]    \n",
    "    labels =[f[:12] for f in listdir(labels) if isfile(join(labels, f))]\n",
    "    predicted = list(set(predictions) & set(labels))\n",
    "    not_predicted = labels\n",
    "    \n",
    "    for element in predicted:\n",
    "        #print(element)\n",
    "        not_predicted.remove(element) \n",
    "\n",
    "    return not_predicted\n",
    "\n",
    "# Check if there is all predictions\n",
    "not_predicted = missed(predictions, labels)\n",
    "all_processes =[]\n",
    "for i in range(models):\n",
    "    all_processes.append('/data1/parallel.py --path {} --model {} --step {} --init {} --conf-t 0.001 --threads {}'.format(path, i, step, init, threads))\n",
    "\n",
    "while len(not_predicted):\n",
    "    # This block of code enables us to call the script from command line.                                                                                \n",
    "    def execute(process):                                                             \n",
    "        os.system(f'python {process}')                                       \n",
    "\n",
    "\n",
    "    process_pool = multiprocessing.Pool(processes = models)                                                        \n",
    "    process_pool.map(execute, all_processes)\n",
    "    \n",
    "    # Check if there is all predictions\n",
    "    not_predicted = missed(predictions, labels) \n",
    "    \n",
    "print(\"Script ran for \", time() - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mAP evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download COCO anotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/COCO/val2017\n",
      "--2021-08-23 12:11:47--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.1.20\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.1.20|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 252907541 (241M) [application/zip]\n",
      "Saving to: ‘annotations_trainval2017.zip’\n",
      "\n",
      "annotations_trainva 100%[===================>] 241.19M  9.34MB/s    in 41s     \n",
      "\n",
      "2021-08-23 12:12:28 (5.94 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n",
      "\n",
      "--2021-08-23 12:12:29--  https://raw.githubusercontent.com/matlab-deep-learning/Object-Detection-Using-Pretrained-YOLO-v2/main/%2Bhelper/coco-classes.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 625 [text/plain]\n",
      "Saving to: ‘coco-classes.txt’\n",
      "\n",
      "coco-classes.txt    100%[===================>]     625  --.-KB/s    in 0s      \n",
      "\n",
      "2021-08-23 12:12:29 (53.7 MB/s) - ‘coco-classes.txt’ saved [625/625]\n",
      "\n",
      "Archive:  annotations_trainval2017.zip\n",
      "  inflating: annotations/instances_train2017.json  \n",
      "  inflating: annotations/instances_val2017.json  \n",
      "  inflating: annotations/captions_train2017.json  \n",
      "  inflating: annotations/captions_val2017.json  \n",
      "  inflating: annotations/person_keypoints_train2017.json  \n",
      "  inflating: annotations/person_keypoints_val2017.json  \n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/COCO/val2017\n",
    "!wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!wget https://raw.githubusercontent.com/matlab-deep-learning/Object-Detection-Using-Pretrained-YOLO-v2/main/%2Bhelper/coco-classes.txt\n",
    "!unzip annotations_trainval2017.zip\n",
    "!mkdir labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f=open(\"annotations/instances_val2017.json\")\n",
    "coco= json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'licenses', 'images', 'annotations', 'categories'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO classes for inference\n",
    "namescoco = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "        'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "        'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "        'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "        'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "        'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "        'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "        'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "        'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 91 classes from COCO's paper\n",
    "names91 = ['empty', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
    "           'traffic light', 'fire hydrant', 'empty', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',\n",
    "           'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'empty', 'backpack', \n",
    "           'umbrella', 'empty', 'empty', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', \n",
    "           'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', \n",
    "           'bottle', 'empty', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', \n",
    "           'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', \n",
    "           'potted plant', 'bed', 'empty', 'dining table', 'empty', 'empty', 'toilet', 'empty', 'tv', 'laptop', \n",
    "           'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', \n",
    "           'empty', 'book','clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate labels in yolo.txt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(coco['images'])):\n",
    "    bbox = []\n",
    "    for j in range(len(coco['annotations'])):\n",
    "        if coco['annotations'][j][\"image_id\"] == coco['images'][i][\"id\"]: \n",
    "            x = coco['annotations'][j][\"bbox\"][0]\n",
    "            y = coco['annotations'][j][\"bbox\"][1]\n",
    "            w =  coco['annotations'][j][\"bbox\"][2]\n",
    "            h = coco['annotations'][j][\"bbox\"][3]\n",
    "            width = coco['images'][i]['width']\n",
    "            height = coco['images'][i]['height']\n",
    "            # Convert class id to 80 classes version\n",
    "            cat = coco['annotations'][j]['category_id']\n",
    "            cls = names91[cat]\n",
    "            cls = namescoco.index(cls)\n",
    "            bbox.append(str(cls) + \" \" + str((x+w/2)/width) + \" \" + str((y+h/2)/height)+\" \" + str(w/width)+\" \" + str(h/height)) \n",
    "            \n",
    "    bbox = '\\n'.join(bbox)\n",
    "    with open('/workspace/COCO/val2017/labels/{}.txt'.format(coco['images'][i][\"file_name\"][:12]), 'w') as f:\n",
    "        f.write(str(bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions on these labels with https://github.com/rafaelpadilla/review_object_detection_metrics\n",
    "# Download all needed files that located under /workspace/COCO/val2017 and use it to calculate mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your results should look like this:\n",
    "\"\"\"\n",
    "COCO METRICS:\n",
    "AP: 0.30763665649871075\n",
    "AP50: 0.5915429205728568\n",
    "AP75: 0.29242076553561813\n",
    "APsmall: 0.10837458746989537\n",
    "APmedium: 0.29600038344642876\n",
    "APlarge: 0.4424104857026416\n",
    "AR1: 0.2567070573097393\n",
    "AR10: 0.4102890791340148\n",
    "AR100: 0.4285984240348608\n",
    "ARsmall: 0.23691939502550804\n",
    "ARmedium: 0.44522361970282825\n",
    "ARlarge: 0.5428652114891923\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
